Evaluation README (for GitHub)
Automatic Quiz Evaluation

This folder contains the evaluation pipeline we used to compare our multimodal quiz generator (video + slide + transcript) against the transcript-only baseline.
Our goal is to quantitatively measure how much the multimodal system improves question quality.

Folder Structure
evaluation/
  eval_compare.py
  eval_compare_quiz_baseline.py    # optional legacy script
  data/
    data_course1/                  # Course A (e.g., CS5xxx)
      quiz/                        # multimodal outputs
      baseline/                    # transcript-only outputs
      references1.jsonl            
    data_course2/                  # Course B (e.g., CS5130)
      quiz/
      baseline/
      references2.jsonl


Each course folder contains:

File / Folder	Description
quiz/	Questions generated by our multimodal model
baseline/	Questions generated using transcript only
references*.jsonl	human reference questions
Evaluation Method

We use standard NLP generation metrics to compare generated questions against a ‚Äúreference‚Äù:

BERTScore (F1) ‚Äì semantic similarity

ROUGE-1 (F1) ‚Äì unigram lexical overlap

ROUGE-L (F1) ‚Äì longest-common-subsequence overlap

BLEU ‚Äì n-gram precision

These provide complementary signals:
semantic correctness, lexical closeness, structural similarity, and phrasing accuracy.

How to Run Evaluation
1. Activate virtual environment

(Using the same environment built during QGEval setup)

source ~/Desktop/qgeval_env/bin/activate

2. Navigate to the evaluation folder
cd evaluation

3. Run evaluation for a course
Course 1
python3 eval_compare.py --course data_course1

Course 2
python3 eval_compare.py --course data_course2

üìä Output Format

Running the script prints:

=== Multimodal vs Reference ===
{ BERTScore, ROUGE-1, ROUGE-L, BLEU }

=== Baseline vs Reference ===
{ BERTScore, ROUGE-1, ROUGE-L, BLEU }

=== Improvements (Multimodal - Baseline) ===
{ Œî_BERTScore, Œî_ROUGE-1, Œî_ROUGE-L, Œî_BLEU }